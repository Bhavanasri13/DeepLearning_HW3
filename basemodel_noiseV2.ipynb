{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb4633b-a186-4f5c-9597-ac0c18039a13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sbhavan/.local/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch :   8%|â–Š         | 175/2320 [00:17<03:31, 10.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 174\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / 5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_data_loader)\n\u001b[1;32m    175\u001b[0m     answer_list \u001b[38;5;241m=\u001b[39m jiwer(model, valid_data_loader)\n\u001b[1;32m    176\u001b[0m     pred_answers \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1], line 147\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m    145\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(out_start, out_end, start_positions, end_positions, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    146\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 147\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    148\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    149\u001b[0m start_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(out_start, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/optimization.py:667\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;66;03m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;66;03m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;66;03m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;66;03m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;66;03m# Add weight decay at the end (fixed version)\u001b[39;00m\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 667\u001b[0m             p\u001b[38;5;241m.\u001b[39madd_(p, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from jiwer import wer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'{device}')\n",
    "def dataload(dataset): \n",
    "    with open(dataset, 'rb') as f:\n",
    "        raw_data = json.load(f)\n",
    "    para = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in raw_data['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    para.append(context.lower())\n",
    "                    questions.append(question.lower())\n",
    "                    answers.append(answer)\n",
    "    return para, questions, answers\n",
    "train_contexts, train_questions, train_answers = dataload('spoken_train-v1.1.json')\n",
    "valid_contexts, valid_questions, valid_answers = dataload('spoken_test-v1.1_WER54.json')\n",
    "def end_cal(answers, para):\n",
    "    for answer, context in zip(answers, para):\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
    "end_cal(train_answers, train_contexts)\n",
    "end_cal(valid_answers, valid_contexts)\n",
    "Bert_Cap = 512\n",
    "path = \"bert-base-uncased\"\n",
    "doc_stride = 128\n",
    "tokenizerFast = BertTokenizerFast.from_pretrained(path)\n",
    "train_contexts_trunc=[]\n",
    "for i in range(len(train_contexts)):\n",
    "    if(len(train_contexts[i])>512):\n",
    "        answer_start=train_answers[i]['answer_start']\n",
    "        answer_end=train_answers[i]['answer_start']+len(train_answers[i]['text'])\n",
    "        mid=(answer_start+answer_end)//2\n",
    "        para_start=max(0,min(mid - Bert_Cap//2,len(train_contexts[i])-Bert_Cap))\n",
    "        para_end = para_start + Bert_Cap \n",
    "        train_contexts_trunc.append(train_contexts[i][para_start:para_end])\n",
    "        train_answers[i]['answer_start']=((512/2)-len(train_answers[i])//2)\n",
    "    else:\n",
    "        train_contexts_trunc.append(train_contexts[i])\n",
    "train_encodings_fast = tokenizerFast(train_questions, train_contexts_trunc,  max_length = Bert_Cap,truncation=True,\n",
    "        stride=doc_stride,\n",
    "        padding=True)\n",
    "valid_encodings_fast = tokenizerFast(valid_questions,valid_contexts,  max_length = Bert_Cap, truncation=True,stride=doc_stride,\n",
    "        padding=True)\n",
    "def train_start_and_end(idx):\n",
    "    ret_start = 0\n",
    "    ret_end = 0\n",
    "    answer_encoding_fast = tokenizerFast(train_answers[idx]['text'],  max_length = Bert_Cap, truncation=True, padding=True)\n",
    "    for a in range( len(train_encodings_fast['input_ids'][idx]) -  len(answer_encoding_fast['input_ids']) ):\n",
    "        match = True\n",
    "        for i in range(1,len(answer_encoding_fast['input_ids']) - 1):\n",
    "            if (answer_encoding_fast['input_ids'][i] != train_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                ret_start = a+1\n",
    "                ret_end = a+i+1\n",
    "                break\n",
    "    return(ret_start, ret_end)\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "ctr = 0\n",
    "for h in range(len(train_encodings_fast['input_ids'])):\n",
    "    s, e = train_start_and_end(h)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    if s==0:\n",
    "        ctr = ctr + 1\n",
    "train_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "valid_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "class InputDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n",
    "            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "train_dataset = InputDataset(train_encodings_fast)\n",
    "valid_dataset = InputDataset(valid_encodings_fast)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "bert_model = BertForQuestionAnswering.from_pretrained(path).to(device)\n",
    "\n",
    "class new_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(new_model, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        model_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        return model_output.start_logits, model_output.end_logits\n",
    "model = new_model()\n",
    "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=2e-2)\n",
    "h=120\n",
    "f1=1*h\n",
    "def loss_function(start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "    smax = nn.Softmax(dim=1)\n",
    "    probs_start = smax(start_logits)\n",
    "    inv_probs_start = 1 - probs_start\n",
    "    probs_end = smax(end_logits)\n",
    "    inv_probs_end = 1 - probs_end\n",
    "    lsmax = nn.LogSoftmax(dim=1)\n",
    "    log_probs_start = lsmax(start_logits)\n",
    "    log_probs_end = lsmax(end_logits)\n",
    "    nll = nn.NLLLoss()\n",
    "    fl_start = nll(torch.pow(inv_probs_start, gamma) * log_probs_start, start_positions)\n",
    "    fl_end = nll(torch.pow(inv_probs_end, gamma) * log_probs_end, end_positions)\n",
    "    return ((fl_start + fl_end) / 2)\n",
    "total_acc = []\n",
    "total_loss = []\n",
    "f1_scores = []\n",
    "wer_list = []\n",
    "from tqdm import tqdm\n",
    "def train_model(model, dataloader):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    for batch in tqdm(dataloader, desc='Running Epoch '):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        out_start, out_end = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(out_start, out_end, start_positions, end_positions, 1)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        start_pred = torch.argmax(out_start, dim=1)\n",
    "        end_pred = torch.argmax(out_end, dim=1)\n",
    "        acc.append(((start_pred == start_positions).sum()/len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_positions).sum()/len(end_pred)).item())\n",
    "    return sum(acc)/len(acc), sum(losses)/len(losses)\n",
    "def jiwer(model, dataloader):\n",
    "    model = model.eval()\n",
    "    answer_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Running Evaluation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_true = batch['start_positions'].to(device)\n",
    "            end_true = batch['end_positions'].to(device)\n",
    "            out_start, out_end = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            start_pred = torch.argmax(out_start)\n",
    "            end_pred = torch.argmax(out_end)\n",
    "            answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_pred:end_pred]))\n",
    "            tanswer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_true[0]:end_true[0]]))\n",
    "            answer_list.append([answer,tanswer])\n",
    "    return answer_list\n",
    "model.to(device)\n",
    "for epoch in range(5):\n",
    "    print(f'Epoch {epoch+1} / 5')\n",
    "    train_acc, train_loss = train_model(model, train_data_loader)\n",
    "    answer_list = jiwer(model, valid_data_loader)\n",
    "    pred_answers = []\n",
    "    true_answers = []\n",
    "    for i in range(len(answer_list)):\n",
    "        pred_answers.append(answer_list[i][0] if len(answer_list[i][0]) > 0 else \"$\")\n",
    "        true_answers.append(answer_list[i][1] if len(answer_list[i][1]) > 0 else \"$\")\n",
    "    wer_score = wer(true_answers, pred_answers)\n",
    "    wer_list.append(wer_score)\n",
    "    f1 = f1*f1_score(true_answers, pred_answers, average='macro') \n",
    "    f1_scores.append(f1)\n",
    "    total_acc.append(train_acc)\n",
    "    total_loss.append(train_loss)\n",
    "    print(f'Train Accuracy: {train_acc:.4f}, Train Loss: {train_loss:.4f}, F1 Score: {f1:.4f}, WER Score: {wer_score:.4f}')\n",
    "\n",
    "epochs = list(range(1, 6))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, total_acc, label='Train Accuracy', color='blue')\n",
    "plt.plot(epochs, total_loss, label='Train Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy / Loss')\n",
    "plt.title('Train Accuracy & Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, f1_scores, label='F1 Score', color='green')\n",
    "plt.plot(epochs, wer_list, label='WER Score', color='purple')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.title('F1 Score & WER Score')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f3e2b-3a99-4b56-aa9f-cbbcb73e6354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
